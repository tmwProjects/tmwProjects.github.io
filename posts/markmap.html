<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
  background-color: #f0f0f0;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.10/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.10/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.10/dist/index.js"></script><script>(()=>{setTimeout(()=>{const{markmap:x,mm:K}=window,P=new x.Toolbar;P.attach(K);const F=P.render();F.setAttribute("style","position:absolute;bottom:20px;right:20px"),document.body.append(F)})})()</script><script>((b,L,T,D)=>{const H=b();window.mm=H.Markmap.create("svg#mindmap",(L||H.deriveOptions)(D),T)})(()=>window.markmap,null,{"content":"Apache Spark","children":[{"content":"Core Components","children":[{"content":"<strong>Spark Core</strong>","children":[{"content":"Grundlegende Architektur von Spark","children":[],"payload":{"tag":"li","lines":"13,14"}},{"content":"Ressourcenverwaltung und Scheduling","children":[],"payload":{"tag":"li","lines":"14,15"}},{"content":"Fehlerbehandlung und Wiederholbarkeit von Jobs","children":[],"payload":{"tag":"li","lines":"15,16"}}],"payload":{"tag":"li","lines":"12,16"}},{"content":"<strong>RDD (Resilient Distributed Dataset)</strong>","children":[{"content":"Unterschiede zu DataFrames und Datasets","children":[],"payload":{"tag":"li","lines":"17,18"}},{"content":"Transformationen und Aktionen","children":[],"payload":{"tag":"li","lines":"18,19"}},{"content":"Vorteile der Fehlertoleranz und Parallelit&#xe4;t","children":[],"payload":{"tag":"li","lines":"19,20"}}],"payload":{"tag":"li","lines":"16,20"}},{"content":"<strong>DAG (Directed Acyclic Graph)</strong>","children":[{"content":"Steuerung der Transformationen und deren Reihenfolge","children":[],"payload":{"tag":"li","lines":"21,22"}},{"content":"Fehlerbehandlung und Neuberechnung von fehlgeschlagenen Tasks","children":[],"payload":{"tag":"li","lines":"22,23"}},{"content":"Einfluss auf die Performance und Optimierung der Berechnungen","children":[],"payload":{"tag":"li","lines":"23,24"}}],"payload":{"tag":"li","lines":"20,24"}},{"content":"<strong>Shared Variables</strong>","children":[{"content":"Einsatz von Broadcast Variables zur Minimierung von Netzwerkverkehr","children":[],"payload":{"tag":"li","lines":"25,26"}},{"content":"Einsatz von Accumulators f&#xfc;r Z&#xe4;hleroperationen in verteilten Berechnungen","children":[],"payload":{"tag":"li","lines":"26,27"}}],"payload":{"tag":"li","lines":"24,27"}},{"content":"<strong>Spark SQL</strong>","children":[{"content":"DataFrames","children":[],"payload":{"tag":"li","lines":"28,29"}},{"content":"Datasets","children":[],"payload":{"tag":"li","lines":"29,30"}},{"content":"<strong>Catalyst Optimizer</strong>","children":[{"content":"Logical Plan Optimization","children":[],"payload":{"tag":"li","lines":"31,32"}},{"content":"Predicate Pushdown","children":[],"payload":{"tag":"li","lines":"32,33"}},{"content":"Join Reordering","children":[],"payload":{"tag":"li","lines":"33,34"}},{"content":"Cost-Based Optimization (CBO)","children":[],"payload":{"tag":"li","lines":"34,35"}},{"content":"Code Generation (Tungsten Engine)","children":[],"payload":{"tag":"li","lines":"35,36"}}],"payload":{"tag":"li","lines":"30,36"}}],"payload":{"tag":"li","lines":"27,36"}},{"content":"<strong>Spark Streaming</strong>","children":[{"content":"DStream (Discretized Stream)","children":[],"payload":{"tag":"li","lines":"37,38"}},{"content":"Structured Streaming","children":[],"payload":{"tag":"li","lines":"38,39"}}],"payload":{"tag":"li","lines":"36,39"}},{"content":"<strong>MLlib (Machine Learning Library)</strong>","children":[{"content":"Algorithms","children":[],"payload":{"tag":"li","lines":"40,41"}},{"content":"Pipelines","children":[],"payload":{"tag":"li","lines":"41,42"}},{"content":"Feature Transformation","children":[],"payload":{"tag":"li","lines":"42,43"}}],"payload":{"tag":"li","lines":"39,43"}},{"content":"<strong>GraphX</strong>","children":[{"content":"Graph Processing","children":[],"payload":{"tag":"li","lines":"44,45"}},{"content":"Pregel API","children":[],"payload":{"tag":"li","lines":"45,47"}}],"payload":{"tag":"li","lines":"43,47"}}],"payload":{"tag":"h2","lines":"11,12"}},{"content":"Cluster Architecture","children":[{"content":"\n<p data-lines=\"48,49\"><strong>Master</strong></p>","children":[{"content":"Verantwortlich f&#xfc;r die Verwaltung von Ressourcen und die Verteilung von Tasks","children":[],"payload":{"tag":"li","lines":"49,50"}},{"content":"Entscheidet, auf welchem Worker-Node Tasks ausgef&#xfc;hrt werden.","children":[],"payload":{"tag":"li","lines":"50,51"}},{"content":"Verbindet und f&#xfc;gt neue Workers hinzu.","children":[],"payload":{"tag":"li","lines":"51,52"}},{"content":"Start:","children":[{"content":"\n<pre data-lines=\"53,56\"><code data-lines=\"53,56\"><span class=\"hljs-meta prompt_\">  &#x24; </span><span class=\"language-bash\">./sbin/start-master.sh</span>\n</code></pre>","children":[],"payload":{"tag":"li","lines":"53,56"}}],"payload":{"tag":"li","lines":"52,56"}}],"payload":{"tag":"li","lines":"48,56"}},{"content":"\n<p data-lines=\"56,57\"><strong>Worker</strong></p>","children":[{"content":"F&#xfc;hren die tats&#xe4;chliche Verarbeitung der Daten aus.","children":[],"payload":{"tag":"li","lines":"57,58"}},{"content":"Halten den Executor, der die Tasks ausf&#xfc;hrt.","children":[],"payload":{"tag":"li","lines":"58,59"}},{"content":"Werden auf Cluster-Nodes ausgef&#xfc;hrt.","children":[],"payload":{"tag":"li","lines":"59,60"}},{"content":"Start:","children":[{"content":"\n<pre data-lines=\"61,64\"><code data-lines=\"61,64\"><span class=\"hljs-meta prompt_\">  &#x24; </span><span class=\"language-bash\">./sbin/start-slave.sh spark://&lt;master-spark-URL&gt;:7077</span>\n</code></pre>","children":[],"payload":{"tag":"li","lines":"61,64"}}],"payload":{"tag":"li","lines":"60,64"}}],"payload":{"tag":"li","lines":"56,64"}},{"content":"\n<p data-lines=\"64,65\"><strong>Driver</strong></p>","children":[{"content":"Verwaltet den Spark-Job, koordiniert die Arbeit der Cluster-Ressourcen","children":[],"payload":{"tag":"li","lines":"65,66"}},{"content":"Stellt eine Verbindung zum Master her und verteilt Tasks","children":[],"payload":{"tag":"li","lines":"66,67"}}],"payload":{"tag":"li","lines":"64,67"}},{"content":"\n<p data-lines=\"67,68\"><strong>Executor</strong></p>","children":[{"content":"F&#xfc;hren Tasks auf Worker-Nodes aus und speichern Ergebnisse","children":[],"payload":{"tag":"li","lines":"68,69"}},{"content":"Jeder Executor f&#xfc;hrt Aufgaben f&#xfc;r den zugewiesenen Task aus und speichert Daten lokal","children":[],"payload":{"tag":"li","lines":"69,70"}}],"payload":{"tag":"li","lines":"67,70"}},{"content":"\n<p data-lines=\"70,71\"><strong>Cluster Manager</strong></p>","children":[{"content":"Verwalten des Ressourcenmanagements im Cluster","children":[],"payload":{"tag":"li","lines":"71,72"}},{"content":"Der Cluster Manager &#xfc;bernimmt das Starten von Executorn und das Zuweisen von Ressourcen","children":[{"content":"\n<p data-lines=\"73,74\">Standalone</p>","children":[{"content":"Es sid keine zus&#xe4;tzlichen Abh&#xe4;ngigkeiten notwendig f&#xfc;r Deployment.","children":[],"payload":{"tag":"li","lines":"74,75"}},{"content":"Der schnellste Weg einen Cluster auszusetzen und laufen zu lassen.","children":[],"payload":{"tag":"li","lines":"75,76"}}],"payload":{"tag":"li","lines":"73,76"}},{"content":"\n<p data-lines=\"76,77\">Hadoop YARN</p>","children":[{"content":"Cluster-Manager f&#xfc;r allgemeinen Zweck.","children":[],"payload":{"tag":"li","lines":"77,78"}},{"content":"YARN-Cluster haben ihre eigenen Abh&#xe4;ngigkeiten.","children":[],"payload":{"tag":"li","lines":"78,79"}},{"content":"Komplexer f&#xfc;r Deployments.","children":[],"payload":{"tag":"li","lines":"79,80"}},{"content":"Unterst&#xfc;tzt weitere zus&#xe4;tzliche Frameworks:","children":[{"content":"Apache Spark","children":[],"payload":{"tag":"li","lines":"81,82"}},{"content":"Apache MapReduce","children":[],"payload":{"tag":"li","lines":"82,83"}},{"content":"Apache Tez","children":[],"payload":{"tag":"li","lines":"83,84"}},{"content":"Apache Flink","children":[],"payload":{"tag":"li","lines":"84,85"}},{"content":"Apache HBase","children":[],"payload":{"tag":"li","lines":"85,86"}},{"content":"Apache Storm","children":[],"payload":{"tag":"li","lines":"86,87"}},{"content":"Apache Drill","children":[],"payload":{"tag":"li","lines":"87,88"}}],"payload":{"tag":"li","lines":"80,88"}}],"payload":{"tag":"li","lines":"76,88"}},{"content":"\n<p data-lines=\"88,89\">Apache Mesos</p>","children":[{"content":"Cluster-Manager f&#xfc;r allgemeinen Zweck.","children":[],"payload":{"tag":"li","lines":"89,90"}},{"content":"M&#xf6;glichkeit f&#xfc;r dynamische Partitionierungen zw. Spark und anderen Big-Data-Frameworks.","children":[],"payload":{"tag":"li","lines":"90,91"}},{"content":"Skalierbar zwischen mehreren Spark-Instanzen.","children":[],"payload":{"tag":"li","lines":"91,92"}}],"payload":{"tag":"li","lines":"88,92"}},{"content":"\n<p data-lines=\"92,93\">Kubernetes</p>","children":[{"content":"Automatisierte Deployments.","children":[],"payload":{"tag":"li","lines":"93,94"}},{"content":"Portierbare Spark-Anwendungen.","children":[],"payload":{"tag":"li","lines":"94,95"}},{"content":"Skaliert des Cluster.","children":[],"payload":{"tag":"li","lines":"95,96"}},{"content":"Vereinfachtes Abh&#xe4;ngigkeits-Management.","children":[],"payload":{"tag":"li","lines":"96,97"}}],"payload":{"tag":"li","lines":"92,97"}},{"content":"\n<p data-lines=\"97,98\"><strong>Spark-Shell</strong></p>","children":[{"content":"Interaktive Shell f&#xfc;r das schnelle Testen von Spark-Funktionen","children":[{"content":"Wird oft genutzt, um Spark-APIs zu lernen und schnelle Experimente durchzuf&#xfc;hren","children":[],"payload":{"tag":"li","lines":"99,100"}}],"payload":{"tag":"li","lines":"98,100"}},{"content":"Verf&#xfc;gbar f&#xfc;r Python und Scala.","children":[],"payload":{"tag":"li","lines":"100,101"}},{"content":"Sobald Spark-Shell gestartet wurde, kann direkt mit Daten gearbeitet werden.","children":[],"payload":{"tag":"li","lines":"101,102"}},{"content":"Starte:<pre data-lines=\"103,106\"><code data-lines=\"103,106\">./bin/pyspark\n</code></pre>","children":[],"payload":{"tag":"li","lines":"102,107"}}],"payload":{"tag":"li","lines":"97,107"}},{"content":"\n<p data-lines=\"107,108\">Local Mode.</p>","children":[{"content":"L&#xe4;uft ohne Cluster-Manager","children":[],"payload":{"tag":"li","lines":"108,109"}},{"content":"Nutzt nur die lokale CPU und den Arbeitsspeicher der Maschine","children":[],"payload":{"tag":"li","lines":"109,110"}},{"content":"Kann in verschiedenen Konfigurationen betrieben werden","children":[],"payload":{"tag":"li","lines":"110,111"}},{"content":"Eignet sich gut f&#xfc;r Entwicklung, Debugging und kleine Datenmengen.","children":[],"payload":{"tag":"li","lines":"111,112"}},{"content":"Starte und konfiguriere:","children":[{"content":"\n<pre data-lines=\"113,118\"><code data-lines=\"113,118\"><span class=\"hljs-meta prompt_\">  &#x24; </span><span class=\"language-bash\">./bin/spark-submit \\\n    --master <span class=\"hljs-built_in\">local</span>[#] \\\n    &lt;additional configuration&gt;</span>\n</code></pre>","children":[],"payload":{"tag":"li","lines":"113,119"}},{"content":"\n<p data-lines=\"119,120\"><strong><code>spark-submit</code></strong></p>","children":[{"content":"\n<p data-lines=\"120,121\">Kommandozeilen-Tool, mit dem Apache Spark-Anwendungen gestartet werden. Es erm&#xf6;glicht die Ausf&#xfc;hrung von Spark-Programmen in verschiedenen Modi (lokal oder auf einem Cluster) und mit verschiedenen Ressourcen-Konfigurationen.</p>","children":[],"payload":{"tag":"li","lines":"120,121"}},{"content":"\n<p data-lines=\"121,122\">Das spark-submit-Skript liest zus&#xe4;tzliche spezifische Konfigurationen in <code>/conf/spark-defaults.conf</code></p>","children":[],"payload":{"tag":"li","lines":"121,122"}},{"content":"\n<p data-lines=\"122,123\">Stellt ggf. eine Verbindung zum zum Cluster Manager her.</p>","children":[],"payload":{"tag":"li","lines":"122,123"}},{"content":"\n<p data-lines=\"123,125\">Alle Anwendungsdateien (einschlie&#xdf;lich JARs oder Python-Dateien) m&#xfc;ssen angegeben werden, damit sie<br>\ndas Treiberprogramm starten und Dateien verteilen k&#xf6;nnen, die im Cluster ausgef&#xfc;hrt werden sollen.</p>","children":[],"payload":{"tag":"li","lines":"123,125"}},{"content":"\n<pre data-lines=\"125,134\"><code data-lines=\"125,134\"> spark-submit \\\n <span class=\"hljs-attr\">--master</span> &lt;modus&gt; \\\n <span class=\"hljs-attr\">--deploy-mode</span> &lt;modus&gt; \\\n <span class=\"hljs-attr\">--num-executors</span> &lt;anzahl&gt; \\\n <span class=\"hljs-attr\">--executor-memory</span> &lt;speicher&gt; \\\n <span class=\"hljs-attr\">--executor-cores</span> &lt;kerne&gt; \\\n mein_spark_programm<span class=\"hljs-selector-class\">.py</span>\n</code></pre>","children":[],"payload":{"tag":"li","lines":"125,135"}},{"content":"\n<p data-lines=\"135,136\"><strong>Wichtige Parameter von `spark-submit</strong>:</p>","children":[{"content":"<strong><code>--master</code></strong>: Definiert, wo die Spark-App l&#xe4;uft (<code>local</code>, <code>yarn</code>, <code>kubernetes</code>, <code>mesos</code>)","children":[{"content":"Beispiel: <code>--master yarn</code>","children":[],"payload":{"tag":"li","lines":"137,138"}}],"payload":{"tag":"li","lines":"136,138"}},{"content":"<strong><code>--deploy-mode</code></strong>: <code>client</code> (Driver l&#xe4;uft lokal) oder <code>cluster</code> (Driver l&#xe4;uft im Cluster)","children":[{"content":"Beispiel: <code>--deploy-mode cluster</code>","children":[],"payload":{"tag":"li","lines":"139,140"}}],"payload":{"tag":"li","lines":"138,140"}},{"content":"<strong><code>--num-executors</code></strong>: Anzahl der Executors im Cluster","children":[{"content":"Beispiel: <code>--num-executors 5</code>","children":[],"payload":{"tag":"li","lines":"141,142"}}],"payload":{"tag":"li","lines":"140,142"}},{"content":"<strong><code>--executor-memory</code></strong>: Speicher pro Executor","children":[{"content":"Beispiel: <code>--executor-memory 4G</code>","children":[],"payload":{"tag":"li","lines":"143,144"}}],"payload":{"tag":"li","lines":"142,144"}},{"content":"<strong><code>--executor-cores</code></strong>: CPU-Kerne pro Executor","children":[{"content":"Beispiel: <code>--executor-cores 2</code>","children":[],"payload":{"tag":"li","lines":"145,146"}}],"payload":{"tag":"li","lines":"144,146"}},{"content":"<strong><code>--driver-memory</code></strong>: Speicher f&#xfc;r den Driver","children":[{"content":"Beispiel: <code>--driver-memory 2G</code>","children":[],"payload":{"tag":"li","lines":"147,148"}}],"payload":{"tag":"li","lines":"146,148"}},{"content":"<strong><code>--conf</code></strong>: Zus&#xe4;tzliche Konfigurationen setzen","children":[{"content":"Beispiel: <code>--conf spark.sql.shuffle.partitions=50</code>","children":[],"payload":{"tag":"li","lines":"149,150"}}],"payload":{"tag":"li","lines":"148,150"}},{"content":"Wildcard (*) nutzen f&#xfc;r ALLE CPU-Kerne. Ansonsten Anzahl (#) angeben.","children":[],"payload":{"tag":"li","lines":"150,153"}}],"payload":{"tag":"li","lines":"135,153"}}],"payload":{"tag":"li","lines":"119,153"}}],"payload":{"tag":"li","lines":"112,153"}}],"payload":{"tag":"li","lines":"107,153"}}],"payload":{"tag":"li","lines":"72,153"}}],"payload":{"tag":"li","lines":"70,153"}},{"content":"\n<p data-lines=\"153,154\"><strong>Task</strong></p>","children":[{"content":"Kleinste Ausf&#xfc;hrungseinheit, die von einem Executor verarbeitet wird","children":[],"payload":{"tag":"li","lines":"154,155"}},{"content":"Besteht aus einer Transformation und/oder einer Aktion","children":[],"payload":{"tag":"li","lines":"155,156"}}],"payload":{"tag":"li","lines":"153,156"}},{"content":"\n<p data-lines=\"156,157\"><strong>Job</strong></p>","children":[{"content":"Besteht aus mehreren Tasks, die von einem Driver koordiniert werden","children":[],"payload":{"tag":"li","lines":"157,158"}},{"content":"Jeder Job besteht aus einem DAG von Tasks, die auf verschiedenen Nodes ausgef&#xfc;hrt werden","children":[],"payload":{"tag":"li","lines":"158,160"}}],"payload":{"tag":"li","lines":"156,160"}}],"payload":{"tag":"h2","lines":"47,48"}},{"content":"Spark Konfigurationen","children":[{"content":"Statische vs. Dynamische Konfiguration","children":[{"content":"<strong>Statische Konfiguration</strong>","children":[{"content":"Wird <strong>vor dem Start</strong> von Spark gesetzt","children":[],"payload":{"tag":"li","lines":"165,166"}},{"content":"&#xc4;nderungen erfordern einen <strong>Neustart</strong>","children":[],"payload":{"tag":"li","lines":"166,167"}},{"content":"Typische Orte:","children":[{"content":"<code>spark-defaults.conf</code>","children":[],"payload":{"tag":"li","lines":"168,169"}},{"content":"<code>spark-env.sh</code>","children":[],"payload":{"tag":"li","lines":"169,170"}},{"content":"<code>log4j.properties</code>","children":[],"payload":{"tag":"li","lines":"170,171"}}],"payload":{"tag":"li","lines":"167,171"}},{"content":"Beispiel (<code>spark-defaults.conf</code>):<pre data-lines=\"172,176\"><code data-lines=\"172,176\">spark.executor.memory 4g\nspark.driver.memory 2g\n</code></pre>","children":[],"payload":{"tag":"li","lines":"171,177"}}],"payload":{"tag":"h4","lines":"164,165"}},{"content":"<strong>Dynamische Konfiguration</strong>","children":[{"content":"Kann <strong>zur Laufzeit</strong> ge&#xe4;ndert werden","children":[],"payload":{"tag":"li","lines":"178,179"}},{"content":"Keine Neustarts erforderlich","children":[],"payload":{"tag":"li","lines":"179,180"}},{"content":"Typische Methoden:","children":[{"content":"<code>spark-submit --conf</code>","children":[],"payload":{"tag":"li","lines":"181,182"}},{"content":"<code>SparkSession.builder.config()</code>","children":[],"payload":{"tag":"li","lines":"182,183"}},{"content":"Dynamische Ressourcenallokation (<code>spark.dynamicAllocation.enabled</code>)","children":[],"payload":{"tag":"li","lines":"183,184"}}],"payload":{"tag":"li","lines":"180,184"}},{"content":"Beispiel (<code>spark-submit</code>):<pre data-lines=\"185,191\"><code class=\"language-bash\">spark-submit \\ \n\t--conf spark.executor.memory=6g \\\n\t--conf spark.executor.cores=2 \\\n\tmy_script.py\n</code></pre>","children":[],"payload":{"tag":"li","lines":"184,191"}},{"content":"Beispiel:<pre data-lines=\"192,201\"><code class=\"language-python\"><span class=\"hljs-keyword\">from</span> pyspark.sql <span class=\"hljs-keyword\">import</span> SparkSession\n\nspark = SparkSession.builder \\\n    .appName(<span class=\"hljs-string\">&quot;DynamicConfigExample&quot;</span>) \\\n    .config(<span class=\"hljs-string\">&quot;spark.executor.memory&quot;</span>, <span class=\"hljs-string\">&quot;6g&quot;</span>) \\\n    .config(<span class=\"hljs-string\">&quot;spark.executor.cores&quot;</span>, <span class=\"hljs-string\">&quot;2&quot;</span>) \\\n    .getOrCreate()\n</code></pre>","children":[],"payload":{"tag":"li","lines":"191,202"}}],"payload":{"tag":"h4","lines":"177,178"}}],"payload":{"tag":"h3","lines":"162,163"}},{"content":"Property Precedence (Priorit&#xe4;t von Spark-Konfigurationen)","children":[{"content":"<strong>Dynamische Konfiguration in der Anwendung</strong>","children":[{"content":"<code>SparkConf</code> oder <code>SparkSession.builder.config()</code>","children":[],"payload":{"tag":"li","lines":"207,208"}},{"content":"&#xdc;berschreibt <strong>alle anderen</strong> Konfigurationen","children":[],"payload":{"tag":"li","lines":"208,210"}}],"payload":{"tag":"h4","lines":"206,207"}},{"content":"<strong><code>spark-submit</code> oder <code>spark-shell</code> Argumente</strong>","children":[{"content":"<code>--conf</code> Optionen &#xfc;berschreiben Werte aus <code>spark-defaults.conf</code>","children":[],"payload":{"tag":"li","lines":"211,213"}}],"payload":{"tag":"h4","lines":"210,211"}},{"content":"<strong><code>spark-defaults.conf</code> (Globale Standardeinstellungen)</strong>","children":[{"content":"Wird verwendet, wenn keine spezifische <code>--conf</code> Option gesetzt wurde","children":[],"payload":{"tag":"li","lines":"214,216"}}],"payload":{"tag":"h4","lines":"213,214"}},{"content":"<strong>Environment Variables (<code>spark-env.sh</code>)</strong>","children":[{"content":"Setzt Umgebungsvariablen, z. B. Speicher- und CPU-Limits f&#xfc;r Worker","children":[],"payload":{"tag":"li","lines":"217,218"}},{"content":"Gilt f&#xfc;r die gesamte Umgebung, nicht nur f&#xfc;r eine einzelne Anwendung","children":[],"payload":{"tag":"li","lines":"218,220"}}],"payload":{"tag":"h4","lines":"216,217"}},{"content":"<strong>Spark-Standardwerte (Niedrigste Priorit&#xe4;t)</strong>","children":[{"content":"Falls nirgends definiert, nutzt Spark eigene Standardwerte","children":[],"payload":{"tag":"li","lines":"221,222"}},{"content":"Beispiel:","children":[{"content":"<code>spark.executor.memory</code>: Standard <strong>1 GB</strong>","children":[],"payload":{"tag":"li","lines":"223,224"}},{"content":"<code>spark.executor.cores</code>: Standard <strong>alle verf&#xfc;gbaren Kerne</strong>","children":[],"payload":{"tag":"li","lines":"224,226"}}],"payload":{"tag":"li","lines":"222,226"}}],"payload":{"tag":"h4","lines":"220,221"}}],"payload":{"tag":"h3","lines":"202,203"}}],"payload":{"tag":"h2","lines":"160,161"}},{"content":"Languages","children":[{"content":"Scala","children":[],"payload":{"tag":"li","lines":"227,228"}},{"content":"Java","children":[],"payload":{"tag":"li","lines":"228,229"}},{"content":"Python (PySpark)","children":[],"payload":{"tag":"li","lines":"229,230"}},{"content":"R (SparkR)","children":[],"payload":{"tag":"li","lines":"230,232"}}],"payload":{"tag":"h2","lines":"226,227"}},{"content":"Data Sources","children":[{"content":"HDFS","children":[],"payload":{"tag":"li","lines":"233,234"}},{"content":"S3","children":[],"payload":{"tag":"li","lines":"234,235"}},{"content":"HBase","children":[],"payload":{"tag":"li","lines":"235,236"}},{"content":"Cassandra","children":[],"payload":{"tag":"li","lines":"236,237"}},{"content":"JDBC","children":[{"content":"JSON","children":[],"payload":{"tag":"li","lines":"238,239"}},{"content":"Parquet","children":[],"payload":{"tag":"li","lines":"239,240"}},{"content":"Avro","children":[],"payload":{"tag":"li","lines":"240,242"}}],"payload":{"tag":"li","lines":"237,242"}}],"payload":{"tag":"h2","lines":"232,233"}},{"content":"Operations","children":[{"content":"<strong>Transformations</strong> (Erzeugen neue RDDs, sind lazy)","children":[{"content":"<strong>Narrow Transformations</strong> (keine oder minimale Shuffles)","children":[{"content":"Map","children":[],"payload":{"tag":"li","lines":"245,246"}},{"content":"Filter","children":[],"payload":{"tag":"li","lines":"246,247"}},{"content":"FlatMap","children":[],"payload":{"tag":"li","lines":"247,248"}}],"payload":{"tag":"li","lines":"244,248"}},{"content":"<strong>Wide Transformations</strong> (erfordern Shuffling der Daten)","children":[{"content":"GroupByKey","children":[],"payload":{"tag":"li","lines":"249,250"}},{"content":"ReduceByKey","children":[],"payload":{"tag":"li","lines":"250,251"}},{"content":"Join","children":[],"payload":{"tag":"li","lines":"251,252"}}],"payload":{"tag":"li","lines":"248,252"}}],"payload":{"tag":"li","lines":"243,252"}},{"content":"<strong>Actions</strong> (L&#xf6;sen die tats&#xe4;chliche Berechnung aus)","children":[{"content":"Collect","children":[],"payload":{"tag":"li","lines":"253,254"}},{"content":"Count","children":[],"payload":{"tag":"li","lines":"254,255"}},{"content":"SaveAsTextFile","children":[],"payload":{"tag":"li","lines":"255,256"}},{"content":"Reduce","children":[],"payload":{"tag":"li","lines":"256,257"}},{"content":"Take","children":[],"payload":{"tag":"li","lines":"257,258"}},{"content":"First","children":[],"payload":{"tag":"li","lines":"258,259"}},{"content":"Foreach","children":[],"payload":{"tag":"li","lines":"259,260"}},{"content":"CountByValue","children":[],"payload":{"tag":"li","lines":"260,262"}}],"payload":{"tag":"li","lines":"252,262"}}],"payload":{"tag":"h2","lines":"242,243"}},{"content":"Performance Optimization","children":[{"content":"<strong>Caching and Persistence</strong> (Speicherlevel: <code>MEMORY_ONLY</code>, <code>MEMORY_AND_DISK</code>)","children":[{"content":"H&#xe4;ufig genutzte Daten k&#xf6;nnen zwischengespeichert werden, um die Leistung zu steigern","children":[],"payload":{"tag":"li","lines":"264,265"}},{"content":"<strong>Persistieren</strong>: Wann und wie man Daten f&#xfc;r wiederholte Berechnungen speichert","children":[],"payload":{"tag":"li","lines":"265,266"}}],"payload":{"tag":"li","lines":"263,266"}},{"content":"<strong>Serialization</strong> (Kryo vs. Java)","children":[{"content":"<strong>Kryo</strong>: Schnellere und kleinere Serialisierung im Vergleich zu Java","children":[],"payload":{"tag":"li","lines":"267,268"}}],"payload":{"tag":"li","lines":"266,268"}},{"content":"<strong>Partitioning</strong> (Aufteilung der Daten f&#xfc;r bessere Parallelverarbeitung)","children":[{"content":"<strong>Types of Partitioning</strong>","children":[{"content":"<strong>Hash Partitioning</strong> (Standard bei <code>reduceByKey</code>, <code>groupBy</code>)","children":[],"payload":{"tag":"li","lines":"270,271"}},{"content":"<strong>Range Partitioning</strong> (Sortiert Daten in Bereiche, gut f&#xfc;r Skew-Probleme)","children":[],"payload":{"tag":"li","lines":"271,272"}},{"content":"<strong>Custom Partitioning</strong> (Eigene Logik durch <code>partitionBy</code>)","children":[],"payload":{"tag":"li","lines":"272,273"}}],"payload":{"tag":"li","lines":"269,273"}},{"content":"<strong>Optimizing Partitioning</strong>","children":[{"content":"<strong>Repartition vs. Coalesce</strong> (<code>repartition(n)</code> f&#xfc;r gleichm&#xe4;&#xdf;ige Verteilung, <code>coalesce(n)</code> f&#xfc;r Zusammenfassung ohne gro&#xdf;e Shuffle-Kosten)","children":[],"payload":{"tag":"li","lines":"274,275"}},{"content":"<strong>Data Locality</strong> (Partitionen nahe an den verarbeitenden Nodes halten)","children":[],"payload":{"tag":"li","lines":"275,276"}},{"content":"<strong>Avoid Small Files</strong> (Zu viele kleine Partitionen sind ineffizient)","children":[],"payload":{"tag":"li","lines":"276,277"}}],"payload":{"tag":"li","lines":"273,277"}}],"payload":{"tag":"li","lines":"268,277"}},{"content":"<strong>Tungsten Engine</strong> (Optimierung der Speicher- und CPU-Nutzung)","children":[{"content":"<strong>Bytecode Generation</strong> (Vermeidung von JVM-Overhead durch direkte Code-Erzeugung)","children":[],"payload":{"tag":"li","lines":"278,279"}},{"content":"<strong>Cache-aware computation</strong> (Effiziente Nutzung von CPU-Caches)","children":[],"payload":{"tag":"li","lines":"279,280"}},{"content":"<strong>Vectorized Processing</strong> (Batch-Verarbeitung von Daten f&#xfc;r bessere Performance)","children":[],"payload":{"tag":"li","lines":"280,281"}},{"content":"<strong>Binary Processing</strong> (Arbeiten mit bin&#xe4;ren Formaten statt Java-Objekten)","children":[],"payload":{"tag":"li","lines":"281,282"}},{"content":"<strong>Memory Management</strong> (Vermeidung von Garbage Collection durch manuelle Speicherverwaltung)","children":[],"payload":{"tag":"li","lines":"282,283"}}],"payload":{"tag":"li","lines":"277,283"}},{"content":"<strong>Shuffle Operations</strong> (Datenbewegung zwischen Nodes, teuer in der Verarbeitung)","children":[{"content":"<strong>Why Shuffle Happens?</strong> (z. B. bei <code>groupByKey</code>, <code>reduceByKey</code>, <code>join</code>)","children":[],"payload":{"tag":"li","lines":"284,285"}},{"content":"<strong>Shuffle Write</strong> (Partitionierte Daten werden auf Festplatte/Netzwerk gespeichert)","children":[],"payload":{"tag":"li","lines":"285,286"}},{"content":"<strong>Shuffle Read</strong> (Andere Worker laden diese Daten zur Verarbeitung)","children":[],"payload":{"tag":"li","lines":"286,287"}},{"content":"<strong>Mitigation Strategies</strong>","children":[{"content":"<strong>Partitioning</strong> (z. B. <code>coalesce</code>, <code>repartition</code> zur Optimierung)","children":[],"payload":{"tag":"li","lines":"288,289"}},{"content":"<strong>ReduceByKey statt GroupByKey</strong> (Minimierung der Shuffle-Gr&#xf6;&#xdf;e)","children":[],"payload":{"tag":"li","lines":"289,290"}},{"content":"<strong>Broadcast Joins</strong> (Vermeidung von teuren Shuffles)","children":[],"payload":{"tag":"li","lines":"290,292"}}],"payload":{"tag":"li","lines":"287,292"}}],"payload":{"tag":"li","lines":"283,292"}}],"payload":{"tag":"h2","lines":"262,263"}},{"content":"Deployment Modes","children":[{"content":"<strong>Local Mode</strong>","children":[{"content":"Eignet sich f&#xfc;r Tests und kleine Datens&#xe4;tze, l&#xe4;uft auf einem einzigen Rechner.","children":[],"payload":{"tag":"li","lines":"294,295"}}],"payload":{"tag":"li","lines":"293,295"}},{"content":"<strong>Cluster Mode</strong>","children":[{"content":"Optimiert f&#xfc;r gro&#xdf;e Datenmengen und f&#xfc;hrt Jobs auf mehreren Maschinen aus.","children":[],"payload":{"tag":"li","lines":"296,297"}},{"content":"<strong>Cluster Manager Deployment</strong>:","children":[{"content":"<strong>YARN-client</strong>: Der Driver l&#xe4;uft auf dem Client, w&#xe4;hrend die Tasks im Cluster laufen.","children":[],"payload":{"tag":"li","lines":"298,299"}},{"content":"<strong>YARN-cluster</strong>: Der Driver l&#xe4;uft im Cluster.","children":[],"payload":{"tag":"li","lines":"299,300"}},{"content":"<strong>Kubernetes</strong>: Verwendung von Kubernetes als Cluster Manager zur Verwaltung von Spark-Anwendungen.","children":[],"payload":{"tag":"li","lines":"300,301"}}],"payload":{"tag":"li","lines":"297,301"}}],"payload":{"tag":"li","lines":"295,301"}},{"content":"<strong>Client Mode</strong>","children":[{"content":"Der Driver l&#xe4;uft auf dem Client, w&#xe4;hrend die Tasks auf dem Cluster ausgef&#xfc;hrt werden.","children":[],"payload":{"tag":"li","lines":"302,304"}}],"payload":{"tag":"li","lines":"301,304"}}],"payload":{"tag":"h2","lines":"292,293"}},{"content":"Ecosystem Integration","children":[{"content":"<strong>Hadoop Ecosystem</strong>","children":[{"content":"HDFS","children":[],"payload":{"tag":"li","lines":"306,307"}},{"content":"YARN","children":[],"payload":{"tag":"li","lines":"307,308"}},{"content":"Hive","children":[],"payload":{"tag":"li","lines":"308,309"}}],"payload":{"tag":"li","lines":"305,309"}},{"content":"<strong>Cloud Services (Spark Deployment)</strong>","children":[{"content":"<strong>AWS (EMR)</strong>: Skalierbare und kosteneffiziente L&#xf6;sung f&#xfc;r Spark auf AWS.","children":[],"payload":{"tag":"li","lines":"310,311"}},{"content":"<strong>Azure (Databricks)</strong>: Managed Spark-Plattform f&#xfc;r fortschrittliche Analysen und maschinelles Lernen.","children":[],"payload":{"tag":"li","lines":"311,312"}},{"content":"<strong>IBM Cloud</strong>: Spark-Integration in IBM Cloud Pak for Data, ideal f&#xfc;r Hybrid-Cloud-Umgebungen und AI-Integration.","children":[],"payload":{"tag":"li","lines":"312,313"}},{"content":"<strong>Azure HDInsight</strong>: Managed Big Data Plattform von Microsoft f&#xfc;r Spark und andere Big Data-Tools.","children":[],"payload":{"tag":"li","lines":"313,314"}},{"content":"<strong>GCP (DataProc)</strong>: Google Cloud&#x2019;s verwaltete Spark- und Hadoop-Dienste f&#xfc;r skalierbare Datenverarbeitung.","children":[],"payload":{"tag":"li","lines":"314,315"}}],"payload":{"tag":"li","lines":"309,315"}},{"content":"<strong>BI Tools</strong>","children":[{"content":"Tableau","children":[],"payload":{"tag":"li","lines":"316,317"}},{"content":"Power BI","children":[],"payload":{"tag":"li","lines":"317,319"}}],"payload":{"tag":"li","lines":"315,319"}}],"payload":{"tag":"h2","lines":"304,305"}},{"content":"Libraries and Extensions","children":[{"content":"SparkR","children":[],"payload":{"tag":"li","lines":"320,321"}},{"content":"PySpark","children":[],"payload":{"tag":"li","lines":"321,322"}},{"content":"GraphFrames","children":[],"payload":{"tag":"li","lines":"322,323"}},{"content":"Koalas","children":[],"payload":{"tag":"li","lines":"323,325"}}],"payload":{"tag":"h2","lines":"319,320"}},{"content":"Spark Cluster Fehlerbehebung","children":[{"content":"Monitoring &amp; Debugging","children":[{"content":"Spark UI","children":[{"content":"Web-Interface zur &#xdc;berwachung von Jobs und deren Performance","children":[],"payload":{"tag":"li","lines":"329,330"}}],"payload":{"tag":"h4","lines":"328,329"}},{"content":"Event Logs","children":[{"content":"Detaillierte Logs, die zur Fehleranalyse und Performanceoptimierung genutzt werden","children":[],"payload":{"tag":"li","lines":"331,332"}}],"payload":{"tag":"h4","lines":"330,331"}},{"content":"Metrics","children":[{"content":"<strong>Ganglia</strong>","children":[],"payload":{"tag":"li","lines":"333,334"}},{"content":"<strong>Graphite</strong>","children":[],"payload":{"tag":"li","lines":"334,335"}}],"payload":{"tag":"h4","lines":"332,333"}},{"content":"Logging","children":[{"content":"<strong>Log4j</strong>","children":[],"payload":{"tag":"li","lines":"336,338"}}],"payload":{"tag":"h4","lines":"335,336"}}],"payload":{"tag":"h3","lines":"327,328"}},{"content":"Benutzercode","children":[{"content":"Fehler","children":[{"content":"<strong>NullPointerException</strong> / <strong>IndexOutOfBoundsException</strong>","children":[],"payload":{"tag":"li","lines":"340,341"}},{"content":"<strong>ClassNotFoundException</strong> / <strong>NoSuchMethodError</strong>","children":[],"payload":{"tag":"li","lines":"341,342"}},{"content":"<strong>SerializationException</strong>","children":[],"payload":{"tag":"li","lines":"342,343"}}],"payload":{"tag":"h5","lines":"339,340"}},{"content":"L&#xf6;sung","children":[{"content":"<strong>Debugging &amp; Logging</strong> (Spark-Logs, Stacktrace pr&#xfc;fen)","children":[],"payload":{"tag":"li","lines":"344,345"}},{"content":"<strong>Richtige Abh&#xe4;ngigkeiten einbinden</strong> (<code>--jars</code>, <code>--packages</code>)","children":[],"payload":{"tag":"li","lines":"345,346"}},{"content":"<strong>RDD-Transformationen pr&#xfc;fen</strong> (Lazy Evaluation beachten)","children":[],"payload":{"tag":"li","lines":"346,347"}},{"content":"<strong>Korrekte Serialisierung sicherstellen</strong> (<code>KryoSerializer</code> verwenden)","children":[],"payload":{"tag":"li","lines":"347,349"}}],"payload":{"tag":"h5","lines":"343,344"}}],"payload":{"tag":"h3","lines":"338,339"}},{"content":"System- &amp; Anwendungskonfiguration","children":[{"content":"Fehler","children":[{"content":"<strong>Inkompatible Spark- / Hadoop-Version</strong>","children":[],"payload":{"tag":"li","lines":"351,352"}},{"content":"<strong>Fehlende Umgebungsvariablen</strong> (<code>SPARK_HOME</code>, <code>HADOOP_CONF_DIR</code>)","children":[],"payload":{"tag":"li","lines":"352,353"}},{"content":"<strong>Unsachgem&#xe4;&#xdf;e Speicherverwaltung</strong>","children":[],"payload":{"tag":"li","lines":"353,354"}}],"payload":{"tag":"h5","lines":"350,351"}},{"content":"L&#xf6;sung","children":[{"content":"<strong>Versionen pr&#xfc;fen</strong> (<code>spark-submit --version</code>, <code>hadoop version</code>)","children":[],"payload":{"tag":"li","lines":"355,356"}},{"content":"<strong>Umgebungsvariablen setzen</strong>","children":[],"payload":{"tag":"li","lines":"356,357"}},{"content":"<strong>Speicherlimits optimieren</strong> (<code>spark.driver.memory</code>, <code>spark.executor.memory</code>)","children":[],"payload":{"tag":"li","lines":"357,359"}}],"payload":{"tag":"h5","lines":"354,355"}}],"payload":{"tag":"h3","lines":"349,350"}},{"content":"Fehlende oder falsche Anwendungsversion","children":[{"content":"Fehler","children":[{"content":"<strong>Nicht gefundene Bibliotheken</strong> (<code>NoClassDefFoundError</code>)","children":[],"payload":{"tag":"li","lines":"361,362"}},{"content":"<strong>Inkompatible JAR-Dateien</strong>","children":[],"payload":{"tag":"li","lines":"362,363"}}],"payload":{"tag":"h5","lines":"360,361"}},{"content":"L&#xf6;sung","children":[{"content":"<strong>Richtige Abh&#xe4;ngigkeiten sicherstellen</strong> (<code>--jars</code>, <code>--files</code>)","children":[],"payload":{"tag":"li","lines":"364,365"}},{"content":"<strong>Versionen von Bibliotheken abgleichen</strong>","children":[],"payload":{"tag":"li","lines":"365,367"}}],"payload":{"tag":"h5","lines":"363,364"}}],"payload":{"tag":"h3","lines":"359,360"}},{"content":"Falsche Ressourcenzuweisung","children":[{"content":"Fehler","children":[{"content":"<strong>OutOfMemoryError</strong>","children":[],"payload":{"tag":"li","lines":"369,370"}},{"content":"<strong>Executor Lost</strong>","children":[],"payload":{"tag":"li","lines":"370,371"}},{"content":"<strong>Task Timeout</strong>","children":[],"payload":{"tag":"li","lines":"371,372"}}],"payload":{"tag":"h5","lines":"368,369"}},{"content":"L&#xf6;sung","children":[{"content":"<strong>Speicher &amp; Kerne anpassen</strong> (<code>spark.executor.memory</code>, <code>spark.executor.cores</code>)","children":[],"payload":{"tag":"li","lines":"373,374"}},{"content":"<strong>Dynamische Allokation aktivieren</strong> (<code>spark.dynamicAllocation.enabled=true</code>)","children":[],"payload":{"tag":"li","lines":"374,375"}},{"content":"<strong>Partitionierung optimieren</strong> (<code>repartition()</code>, <code>coalesce()</code>)","children":[],"payload":{"tag":"li","lines":"375,377"}}],"payload":{"tag":"h5","lines":"372,373"}}],"payload":{"tag":"h3","lines":"367,368"}},{"content":"Netzwerkprobleme","children":[{"content":"Fehler","children":[{"content":"<strong>Timeouts bei Daten&#xfc;bertragung</strong>","children":[],"payload":{"tag":"li","lines":"379,380"}},{"content":"<strong>Verlorene Cluster-Knoten</strong>","children":[],"payload":{"tag":"li","lines":"380,381"}},{"content":"<strong>Verbindungsprobleme zwischen Master &amp; Worker</strong>","children":[],"payload":{"tag":"li","lines":"381,382"}}],"payload":{"tag":"h5","lines":"378,379"}},{"content":"L&#xf6;sung","children":[{"content":"<strong>Netzwerkkonfiguration pr&#xfc;fen</strong> (<code>spark.network.timeout</code>)","children":[],"payload":{"tag":"li","lines":"383,384"}},{"content":"<strong>Heartbeat-Intervalle anpassen</strong> (<code>spark.executor.heartbeatInterval</code>)","children":[],"payload":{"tag":"li","lines":"384,385"}},{"content":"<strong>Fehlertoleranzmechanismen nutzen</strong> (<code>spark.task.maxFailures</code>)","children":[],"payload":{"tag":"li","lines":"385,388"}}],"payload":{"tag":"h5","lines":"382,383"}}],"payload":{"tag":"h3","lines":"377,378"}}],"payload":{"tag":"h2","lines":"325,326"}}],"payload":{"tag":"h1","lines":"9,10"}},{"theme":"dark", "colorFreezeLevel":2,"maxWidth":300,"activeNode":{},"initialExpandLevel":2})</script>
</body>
</html>
